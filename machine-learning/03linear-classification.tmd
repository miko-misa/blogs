# 線形クラス分類
## 導入
回帰分析ではある説明変数から目的変数の値を予測したが、**クラス分類**（classification）ではある説明変数からそのデータが所属するクラスを予測する。例えば、メールがスパムメールか否かを判定したり、画像に写っている物体が猫か犬かを判定したりする問題がクラス分類に該当する。つまり、回帰とは違い最終的な出力は離散的な値となる。しかし、計算が発生する以上、連続的な値を扱う必要があるため、クラス分類ではモデル（分類する計算機械）は連続的な値を出力し、その値を基にクラスを判定する仕組みを採用することが多い。

説明変数を回帰と同様に$x in RR^d$とし、クラスの有限個の集合を$cal(C) = {c_1, c_2, ..., c_K}$とする。ここで、$K$はクラスの数である。$y$を$x$に対応する目的変数であり、$y in cal(C)$を満たすとする。クラス分類の目的は、説明変数$x$を入力として目的変数$y$を予測する関数$f: RR^d 	-> cal(C)$を構築することである。

## 線形クラス分類モデル
線形クラス分類モデルは、各クラス$c_k in cal(C)$に対応する別々の重みベクトル$bold(w)_k in RR^d$を用意し、重回帰分析と同様に値$z$を以下のように計算する。
$$
z_k = bold(w)_k^top bold(x) quad (k in cal(C))
$$
これで計算された値$z_k$を各クラス$c_k$に対応させ、最も大きな値を出力したクラスを予測結果とする。つまり、予測関数$f$は以下のように定義される。
$$
f(bold(x)) = op("argmax", limits: #true)_(k in cal(C)) z_k = op("argmax", limits: #true)_(k in cal(C)) bold(w)_k^top bold(x)
$$

ここで$op("argmax", limits: #true)_(k in cal(C))$は、添字$k$がクラス集合$cal(C)$を動くときに、$z_k$の値が最大となる添字$k$を返す関数である。

## ロジスティック回帰モデル
線形クラス分類モデルは計算が単純である一方で、各クラスに対応する値$z_k$の差が非常に大きくなると、予測が不安定になるという欠点がある。例えば、あるデータ点に対して、クラス$c_1$に対応する値$z_1 = 1000$、クラス$c_2$に対応する値$z_2 = 999$、クラス$c_3$に対応する値$z_3 = -1000$であったとする。この場合、クラス$c_1$が予測されるが、$z_1$と$z_2$の差は非常に小さいため、わずかなノイズで予測結果が変わってしまう可能性がある。

この問題を解決するために、ロジスティック回帰モデルでは各クラス$c_k$に対応する値$z_k$を確率に変換する**ソフトマックス関数**（softmax function）を用いる。ソフトマックス関数$sigma: RR^K 	-> RR^K$は以下のように定義される。
$$
[sigma(bold(z))]_k &= frac(exp(z_k), sum_(j=1)^K exp(z_j)) quad (k = 1, 2, ..., K) \
$$
ここで、$bold(z)$は$z_k$を集めた$K$次元のベクトルである。これは、各クラス$c_k$に対応する値$z_k$を指数関数で変換し、その総和で割ることで、各クラスに対応する確率を計算する関数である。ソフトマックス関数の出力は常に0から1の範囲に収まり、全てのクラスの確率の総和は1になる。つまり、説明変数$bold(x)$の元でその予測$hat(y)$が$c_k in cal(C)$に属する確率と見做せる。
$$
P(hat(y) = c_k | bold(x)) = [sigma(bold(z))]_k
$$
さて、$bold(z)$は以下のようにまとめて計算できる。
$$
bold(z) = vec(bold(w)_1^top bold(x), bold(w)_2^top bold(x), dots.v, bold(w)_K^top bold(x)) = vec(bold(w)_1^top, bold(w)_2^top, dots.v, bold(w)_K^top) bold(x)
$$
ここで、$vec(bold(w)_1^top, bold(w)_2^top, dots.v, bold(w)_K^top)$は各クラスに対応する重みベクトルを行に持つ$K times d$行列でこれを$bold(W)$と表すことにする。すると、ソフトマックス関数の出力は以下のように計算できる。
$$
bold(p) = sigma(bold(z)) = sigma(bold(W) bold(x))
$$
そして、そのうち最も大きな確率を与えるクラスを予測結果とする。つまり、予測関数$f$は以下のように定義される。
$$
f(bold(x)) = op("argmax", limits: #true)_(k in cal(C)) [bold(p)]_k = op("argmax", limits: #true)_(k in cal(C)) [sigma(bold(W) bold(x))]_k
$$

### ソフトマックスの性質
ソフトマックス関数$sigma(bold(z))$の出力は以下のの性質を持つ。
1. 各要素は0以上である：$[sigma(bold(z))]_k >= 0$ for all $k$
2. 全ての要素の和は1である：$sum_(k=1)^K [sigma(bold(z))]_k = 1$

加えて、実装上の注意点として、ソフトマックス関数は指数関数を用いるため、$z_k$の値が大きい場合にオーバーフローが発生する可能性がある。この問題を回避するために、$bold(z)$の各要素からその最大値を引くことで数値的に安定化させることが一般的である。

::: box 【定理】ソフトマックス関数は定数の加法に不変
#### 主張
ソフトマックス関数$sigma(bold(z))$は、入力ベクトル$bold(z)$の各要素に定数$c in RR$を加えた場合でも出力が変わらない。
$$
[sigma(bold(z) + c bold(1))]_k = [sigma(bold(z))]_k quad (k = 1, 2, ..., K)
$$
ここで、$bold(1)$は全ての要素が1である$K$次元ベクトルである。

#### 証明
$$
[sigma(bold(z) + c bold(1))]_k &= frac(exp(z_k + c), sum_(j=1)^K exp(z_j + c)) \
&= frac(exp(z_k) exp(c), sum_(j=1)^K exp(z_j) exp(c)) \
&= frac(exp(z_k), sum_(j=1)^K exp(z_j)) \
&= [sigma(bold(z))]_k
$$
:::

この定理より、$c = - max_(k=1, 2, ..., K) z_k$とすることで、ソフトマックス関数の数値的安定化が可能となる。

また、のちにソフトマックス関数の微分を用いるのでここで導出しておくことにする。ソフトマックス関数$sigma$は$RR^K 	-> RR^K$の関数であるのでヤコビアンを計算する。定義より$sum_(k=1)^K exp(z_k) = S$とおくと、

$$
[(diff sigma)/(diff bold(z))]_(i j) &= diff/(diff z_j) [sigma(bold(z))]_i \
&= diff/(diff z_j) ( frac(exp(z_i), sum_(k=1)^K exp(z_k)) ) \
&= cases(
(S diff/(diff z_j) exp(z_i) - exp(z_i) diff/(diff z_j) S) / S^2 quad & (i = j),
(- exp(z_i) diff/(diff z_j) S) / S^2 quad & (i eq.not j)
)\
&= cases(
(exp(z_i) S - exp(z_i) exp(z_i)) / S^2 quad & (i = j),
(- exp(z_i) exp(z_j)) / S^2 quad & (i eq.not j)
)\
$$
ここで、
$$
[sigma(bold(z))]_k &= frac(exp(z_k), S)
$$
であることを思い出すと、
$$
[(diff sigma)/(diff bold(z))]_(i j) &= cases(
[sigma(bold(z))]_i (1 - [sigma(bold(z))]_i) quad & (i = j),
- [sigma(bold(z))]_i [sigma(bold(z))]_j quad & (i eq.not j)
)\
&= delta_(i j) [sigma(bold(z))]_i - [sigma(bold(z))]_i [sigma(bold(z))]_j
$$
となる。ここで、$delta_(i j)$はクロネッカーのデルタであり、$i = j$のときに1、そうでないときに0となる。
## one-hot表現
クラス分類問題では、目的変数$y$を**one-hot表現**（one-hot representation）で表すことが多い。今ままで$y$はクラス集合$cal(C)$の要素として表されていたが、one-hot表現では$y$を$K$次元のベクトル$bold(y) in RR^K$で表す。具体的には、目的変数$y$がクラス$c_k$に属する場合、one-hot表現では以下のように表される。
$$
[bold(y)]_j &= cases(
1 quad & (j = k),
0 quad & (j eq.not k)
)\
bold(y) &= (0, 0, dots.c, 1, dots.c, 0)^top in RR^K
$$
すると、目的変数はスカラー値からベクトル値に変わる。この表現は、ある種の確率分布を表し、正解クラスが$1$で残りが$0$となるため、ソフトマックス関数の出力と比較しやすくなるという利点がある。今後はこのようにして目的変数を表すこととする。

すると、予測関数$f$はベクトルからベクトルを出力する関数となり、以下のように修正される。
$$
f(bold(x)) = sigma(bold(W) bold(x))
$$
つまり、目的変数$bold(y)$が確率分布で$K$次元ベクトルになったので予測関数はソフトマックス関数の出力そのものにすることができる。

## 最尤推定

::: box 【定義】尤度
ある確率モデルが与えられたとき、観測データ$cal(D) = {(bold(x)_(i), bold(y)_(i))}_{i=1}^n$が得られたとする。このとき、モデルのパラメータ$bold(W)$に対する**尤度**（likelihood）$L(bold(W))$は以下のように定義される。
$$
L(bold(W)) = product_(i=1)^n P(bold(y)_(i) | bold(x)_(i); bold(W))
$$
ここで、$P(bold(y)_(i) | bold(x)_(i); bold(W))$は説明変数$bold(x)_(i)$の元で目的変数$bold(y)_(i)$が観測される確率を表す。
:::

つまり尤度とはあるモデルとパラメータの元で観測データが得られる確率を表す。観測データはすでに得られているので、尤度を最大化（1に近づける）するようなパラメータ$bold(W)$を求めることが目的となる。この方法を**最尤推定**（maximum likelihood estimation）と呼ぶ。

さて、ロジスティック回帰モデルにおける尤度を計算してみよう。ある説明変数$bold(x)_i$において、パラメータ$W$の元で目的変数$bold(y)_i$が観測される確率は以下のように計算できる。
$$
P(hat(bold(y)) = bold(y)_(i) | bold(x)_(i); bold(W)) = [sigma(bold(W) bold(x)_(i))]_k
$$
ここで、$k$は目的変数$bold(y)_(i)$が1を持つ添字、つまり正解クラスである。目的変数$bold(y)_(i)$がone-hot表現であることを思い出すと、上式は以下のように書き換えられる。
$$
P(hat(bold(y)) = bold(y)_(i) | bold(x)_(i); bold(W)) = product_(k=1)^K [sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k)
$$
なぜなら
$$
[sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k) = cases(
[sigma(bold(W) bold(x)_(i))]_k quad & ([bold(y)_(i)]_k = 1),
1 quad & ([bold(y)_(i)]_k = 0)
)\
$$
であるからである。したがって、尤度$L(bold(W))$は以下のように計算できる。
$$
L(bold(W)) &= product_(i=1)^n P(hat(bold(y)) = bold(y)_(i) | bold(x)_(i); bold(W)) \
&= product_(i=1)^n product_(k=1)^K [sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k)
$$
最尤推定ではこの尤度$L(bold(W))$を最大化するようなパラメータ$bold(W)^ast$を求める。

さて、これを用いて損失関数を考えよう。このままだと指数が多重積分の形になっており計算が煩雑になるため、対数を取って対数尤度関数$log L(bold(W))$を考えることにする。対数関数は単調増加関数であり、正の数に対して1対1で対応するため、対数尤度関数を最大化することは尤度関数を最大化することと同値である。また、損失関数は最小化問題として定義されることが多いため、対数尤度関数の負の値を損失関数とする。以上で新しい損失関数$cal(L)_"CE" (bold(W))$を以下のように定義できる。
$$
cal(L)_"CE" (bold(W)) &= - log L(bold(W)) \
&= - log( product_(i=1)^n product_(k=1)^K [sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k) ) \
&= - sum_(i=1)^n sum_(k=1)^K [bold(y)_(i)]_k log [sigma(bold(W) bold(x)_(i))]_k
$$
ここで、$cal(L)_"CE" (bold(W))$は**交差エントロピー損失関数**（cross-entropy loss function）と呼ばれ、「エントロピー」という情報理論の概念からも導出できる。交差エントロピー損失関数は、モデルの予測分布$sigma(bold(W) bold(x)_(i))$と実際の分布$bold(y)_(i)$との間の差異を測る指標であり、これを最小化することはモデルの予測が実際のデータに近づくことを意味する。交差エントロピーは以下のように分解可能である。
$$
cal(l)^("CE")_i (bold(W)) :&= - sum_(k=1)^K [bold(y)_(i)]_k log [sigma(bold(W) bold(x)_(i))]_k\
cal(L)_"CE" (bold(W)) &= sum_(i=1)^n cal(l)^("CE")_i (bold(W))
$$
ここで、$cal(L)^("CE")_i (bold(W))$は$i$番目のデータ点に対する交差エントロピー損失を表す。したがって、交差エントロピーは分解可能目的である。

## 勾配降下法による最適化
交差エントロピー損失関数$cal(L)_"CE" (bold(W))$を最小化するパラメータ$bold(W)^ast$を求めるために、勾配降下法を用いる。まず、交差エントロピー損失関数の勾配を計算する必要がある。$bold(W)$で微分してもよいが、ここでは$bold(w)_k$で微分することにする。また、確率的勾配降下法を念頭に置いて、$i$番目のデータ点に対する交差エントロピー損失$cal(l)^("CE")_i (bold(W))$の勾配を計算する。
$$
diff/(diff bold(w)_k) cal(l)^("CE")_i (bold(W)) &= diff/(diff bold(w)_k) ( - sum_(j=1)^K [bold(y)_(i)]_j log [sigma(bold(W) bold(x)_(i))]_j ) \
&= - sum_(j=1)^K [bold(y)_(i)]_j diff/(diff bold(w)_k) ( log [sigma(bold(W) bold(x)_(i))]_j ) \
&= - sum_(j=1)^K [bold(y)_(i)]_j frac(1, [sigma(bold(W) bold(x)_(i))]_j) diff/(diff bold(w)_k) ( [sigma(bold(W) bold(x)_(i))]_j ) \
$$
ここで、$bold(z) = bold(W) bold(x)_(i)$とおくと、
$$
bold(z) = vec(bold(w)_1^top bold(x)_(i), bold(w)_2^top bold(x)_(i), dots.v, bold(w)_K^top bold(x)_(i))
$$
であるから、$bold(w)_k$は$z_k$にのみ影響を与える。ここで、ソフトマックス関数の微分より
$$
(diff sigma_j)/(diff z_k) = [(diff sigma)/(diff bold(z))]_(j k) = delta_(j k) [sigma(bold(z))]_j - [sigma(bold(z))]_j [sigma(bold(z))]_k
$$
であるから、
$$
diff/(diff bold(w)_k) ( [sigma(bold(W) bold(x)_(i))]_j ) &= diff/(diff bold(w)_k) ( [sigma(bold(z))]_j ) \
&= (diff sigma_j)/(diff z_k) diff/(diff bold(w)_k) z_k \
&= ( delta_(j k) [sigma(bold(z))]_j - [sigma(bold(z))]_j [sigma(bold(z))]_k ) diff/(diff bold(w)_k) ( bold(w)_k^top bold(x)_(i) ) \
&= ( delta_(j k) [sigma(bold(z))]_j - [sigma(bold(z))]_j [sigma(bold(z))]_k ) bold(x)_(i)
$$
である。また、$hat(bold(y))_(i) = sigma(bold(W) bold(x)_(i))$なので
$$
diff/(diff bold(w)_k) cal(l)^("CE")_i (bold(W)) &= - sum_(j=1)^K [bold(y)_(i)]_j frac(1, [hat(bold(y))_(i)]_j) ( delta_(j k) [hat(bold(y))_(i)]_j - [hat(bold(y))_(i)]_j [hat(bold(y))_(i)]_k ) bold(x)_(i) \
&= - sum_(j=1)^K [bold(y)_(i)]_j ( delta_(j k) - [hat(bold(y))_(i)]_k ) bold(x)_(i) \
&= - [bold(y)_i]_k (1 - [hat(bold(y))_(i)]_k ) bold(x)_(i) + sum_(j=1, j eq.not k)^K [bold(y)_(i)]_j [hat(bold(y))_(i)]_k bold(x)_(i) \
&= - [bold(y)_(i)]_k bold(x)_(i) + [hat(bold(y))_(i)]_k bold(x)_(i) sum_(j=1)^K [bold(y)_(i)]_j  \
&= ( [hat(bold(y))_(i)]_k - [bold(y)_(i)]_k ) bold(x)_(i) quad (because sum_(j=1)^K [bold(y)_(i)]_j = 1)
$$
行列の微分は以下のように定義される。
::: box 【定義】行列の微分
行列$bold(W) in RR^(K times d)$で定義された関数$cal(F): RR^(K times d) 	-> RR$の微分$(diff cal(F))/(diff bold(W))$は、各行$bold(w)_k$で微分したベクトルを行に持つ$K times d$行列として定義される。
$$
(diff cal(F))/(diff bold(W)) = vec( ((diff cal(F))/(diff bold(w)_1) )^top, ((diff cal(F))/(diff bold(w)_2) )^top, dots.v, ((diff cal(F))/(diff bold(w)_K) )^top ) in RR^(K times d)
$$
:::
したがって、交差エントロピー損失関数$cal(l)^("CE")_i (bold(W))$の勾配は以下のように表される。
$$
nabla cal(l)^("CE")_i (bold(W)) = (diff cal(l)^("CE")_i)/(diff bold(W)) &= vec( ((diff cal(l)^("CE")_i)/(diff bold(w)_1) )^top, ((diff cal(l)^("CE")_i)/(diff bold(w)_2) )^top, dots.v, ((diff cal(l)^("CE")_i)/(diff bold(w)_K) )^top ) in RR^(K times d)\
&= vec( (( [hat(bold(y))_(i)]_1 - [bold(y)_(i)]_1 ) bold(x)_(i))^top, (( [hat(bold(y))_(i)]_2 - [bold(y)_(i)]_2 ) bold(x)_(i))^top, dots.v, (( [hat(bold(y))_(i)]_K - [bold(y)_(i)]_K ) bold(x)_(i))^top ) \
&= vec( ( [hat(bold(y))_(i)]_1 - [bold(y)_(i)]_1 ) bold(x)_(i)^top, ( [hat(bold(y))_(i)]_2 - [bold(y)_(i)]_2 ) bold(x)_(i)^top, dots.v, ( [hat(bold(y))_(i)]_K - [bold(y)_(i)]_K ) bold(x)_(i)^top )\
&= vec( [hat(bold(y))_(i)]_1 - [bold(y)_(i)]_1, [hat(bold(y))_(i)]_1 - [bold(y)_(i)]_1, dots.v, [hat(bold(y))_(i)]_K - [bold(y)_(i)]_K) bold(x)_(i)^top \
&= ( hat(bold(y))_(i) - bold(y)_(i) ) bold(x)_(i)^top
$$
確率的勾配降下法の更新式は以下のようになる。
$$
bold(w)^((t+1))_k &= bold(w)^((t))_k - eta diff/(diff bold(w)_k) cal(l)^("CE")_i (bold(W^((t))))
$$
これをそれぞれを列ベクトルに転置して並べて行列を作ると
$$
vec( (bold(w)^((t+1))_1)^top, (bold(w)^((t+1))_2)^top, dots.v, (bold(w)^((t+1))_K)^top ) &= vec( (bold(w)^((t))_1)^top, (bold(w)^((t))_2)^top, dots.v, (bold(w)^((t))_K)^top ) - eta vec( ((diff cal(l)^("CE")_i)/(diff bold(w)_1) )^top, ((diff cal(l)^("CE")_i)/(diff bold(w)_2) )^top, dots.v, ((diff cal(l)^("CE")_i)/(diff bold(w)_K) )^top ) \
<=> bold(W^((t+1))) &= bold(W^((t))) - eta ( hat(bold(y))_(i) - bold(y)_(i) ) bold(x)_(i)^top
$$

## 評価
モデルの評価には、正解率（accuracy）や適合率（precision）、再現率（recall）、F1スコアなどの指標が用いられる。これらの指標は、モデルの予測結果と実際のクラスラベルを比較することで計算される。$C_(i j)$を正解クラスが$c_i$で予測クラスが$c_j$であるデータ点の数とすると、**混同行列**（confusion matrix）$C$は以下のように表される。
$$
[C]_(i j) = C_(i j) quad (i, j = 1, 2, ..., K)
$$
この表あるいは値を使って以下の3つの値を定義できる。

- $"TP"_k = C_(k k)$：クラス$k$に正しく分類されたデータ点の数（True Positive）
- $"FP"_k = sum_(i=1, i eq.not k)^K C_(i k)$：クラス$k$に誤って分類されたデータ点の数（False Positive）
- $"FN"_k = sum_(j=1, j eq.not k)^K C_(k j)$：クラス$k$に分類されるべきであったが誤って他のクラスに分類されたデータ点の数（False Negative）

これらを用いて、以下の指標を定義できる。

::: box 【定義】クラス分類の評価指標
- **正解率**（accuracy）：全データ点に対する正しく分類されたデータ点の割合
$$
text("Accuracy") := frac(sum_(k=1)^K "TP"_k, sum_(i=1)^K sum_(j=1)^K C_(i j))
$$
- **適合率**（precision）：クラス$k$に分類されたデータ点のうち、実際にクラス$k$であったデータ点の割合
$$
text("Precision")_k := frac("TP"_k, "TP"_k + "FP"_k)
$$
- **再現率**（recall）：実際にクラス$k$であったデータ点のうち、正しくクラス$k$に分類されたデータ点の割合
$$
text("Recall")_k := frac("TP"_k, "TP"_k + "FN"_k)
$$
- **F1スコア**（F1 score）：適合率と再現率の調和平均
$$
text("F1 Score")_k :=& frac(2 text("Precision")_k text("Recall")_k, text("Precision")_k + text("Recall")_k)\
=& frac(2 "TP"_k, 2 "TP"_k + "FP"_k + "FN"_k)
$$
:::

これらの指標を用いて、モデルの性能を評価し、必要に応じてモデルの改善を行うことができる。ただ、これらは各クラスごとに計算されるため、全体の評価を行う場合はマクロ平均やマイクロ平均などの手法を用いることがある。

::: box 【定義】マクロ平均とマイクロ平均
- **マクロ平均**（macro average）：各クラスの指標を単純に平均したもの
$$
text("Macro Precision") &:= frac(1, K) sum_(k=1)^K text("Precision")_k \
text("Macro Recall") &:= frac(1, K) sum_(k=1)^K text("Recall")_k \
text("Macro F1 Score") &:= frac(1, K) sum_(k=1)^K text("F1 Score")_k
$$
- **マイクロ平均**（micro average）：全クラスのTP、FP、FNを合計してから指標を計算したもの
$$
text("Micro Precision") &:= frac(sum_(k=1)^K "TP"_k, sum_(k=1)^K ("TP"_k + "FP"_k)) \
text("Micro Recall") &:= frac(sum_(k=1)^K "TP"_k, sum_(k=1)^K ("TP"_k + "FN"_k)) \
text("Micro F1 Score") &:= frac(2 sum_(k=1)^K "TP"_k, 2 sum_(k=1)^K "TP"_k + sum_(k=1)^K "FP"_k + sum_(k=1)^K "FN"_k)
$$
:::