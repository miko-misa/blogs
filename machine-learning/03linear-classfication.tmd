# 線形クラス分類
## 導入
回帰分析ではある説明変数から目的変数の値を予測したが、**クラス分類**（classification）ではある説明変数からそのデータが所属するクラスを予測する。例えば、メールがスパムメールか否かを判定したり、画像に写っている物体が猫か犬かを判定したりする問題がクラス分類に該当する。つまり、回帰とは違い最終的な出力は離散的な値となる。しかし、計算が発生する以上、連続的な値を扱う必要があるため、クラス分類ではモデル（分類する計算機械）は連続的な値を出力し、その値を基にクラスを判定する仕組みを採用することが多い。

説明変数を回帰と同様に$x in RR^d$とし、クラスの有限個の集合を$cal(C) = {c_1, c_2, ..., c_K}$とする。ここで、$K$はクラスの数である。$y$を$x$に対応する目的変数であり、$y in cal(C)$を満たすとする。クラス分類の目的は、説明変数$x$を入力として目的変数$y$を予測する関数$f: RR^d 	-> cal(C)$を構築することである。

## 線形クラス分類モデル
線形クラス分類モデルは、各クラス$c_k in cal(C)$に対応する別々の重みベクトル$bold(w)_k in RR^d$を用意し、重回帰分析と同様に値$z$を以下のように計算する。
$$
z_k = bold(w)_k^top bold(x) quad (k in cal(C))
$$
これで計算された値$z_k$を各クラス$c_k$に対応させ、最も大きな値を出力したクラスを予測結果とする。つまり、予測関数$f$は以下のように定義される。
$$
f(bold(x)) = op("argmax", limits: #true)_(k in cal(C)) z_k = op("argmax", limits: #true)_(k in cal(C)) bold(w)_k^top bold(x)
$$

ここで$op("argmax", limits: #true)_(k in cal(C))$は、添字$k$がクラス集合$cal(C)$を動くときに、$z_k$の値が最大となる添字$k$を返す関数である。

## ロジスティック回帰モデル
線形クラス分類モデルは計算が単純である一方で、各クラスに対応する値$z_k$の差が非常に大きくなると、予測が不安定になるという欠点がある。例えば、あるデータ点に対して、クラス$c_1$に対応する値$z_1 = 1000$、クラス$c_2$に対応する値$z_2 = 999$、クラス$c_3$に対応する値$z_3 = -1000$であったとする。この場合、クラス$c_1$が予測されるが、$z_1$と$z_2$の差は非常に小さいため、わずかなノイズで予測結果が変わってしまう可能性がある。

この問題を解決するために、ロジスティック回帰モデルでは各クラス$c_k$に対応する値$z_k$を確率に変換する**ソフトマックス関数**（softmax function）を用いる。ソフトマックス関数$sigma: RR^K 	-> RR^K$は以下のように定義される。
$$
[sigma(bold(z))]_k &= frac(exp(z_k), sum_(j=1)^K exp(z_j)) quad (k = 1, 2, ..., K) \
$$
ここで、$bold(z)$は$z_k$を集めた$K$次元のベクトルである。これは、各クラス$c_k$に対応する値$z_k$を指数関数で変換し、その総和で割ることで、各クラスに対応する確率を計算する関数である。ソフトマックス関数の出力は常に0から1の範囲に収まり、全てのクラスの確率の総和は1になる。つまり、説明変数$bold(x)$の元でその予測$hat(y)$が$c_k in cal(C)$に属する確率と見做せる。
$$
P(hat(y) = c_k | bold(x)) = [sigma(bold(z))]_k
$$
さて、$bold(z)$は以下のようにまとめて計算できる。
$$
bold(z) = vec(bold(w)_1^top bold(x), bold(w)_2^top bold(x), dots.v, bold(w)_K^top bold(x)) = vec(bold(w)_1^top, bold(w)_2^top, dots.v, bold(w)_K^top) bold(x)
$$
ここで、$vec(bold(w)_1^top, bold(w)_2^top, dots.v, bold(w)_K^top)$は各クラスに対応する重みベクトルを行に持つ$K times d$行列でこれを$bold(W)$と表すことにする。すると、ソフトマックス関数の出力は以下のように計算できる。
$$
bold(p) = sigma(bold(z)) = sigma(bold(W) bold(x))
$$
そして、そのうち最も大きな確率を与えるクラスを予測結果とする。つまり、予測関数$f$は以下のように定義される。
$$
f(bold(x)) = op("argmax", limits: #true)_(k in cal(C)) [bold(p)]_k = op("argmax", limits: #true)_(k in cal(C)) [sigma(bold(W) bold(x))]_k
$$

### ソフトマックスの性質
ソフトマックス関数$sigma(bold(z))$の出力は以下のの性質を持つ。
1. 各要素は0以上である：$[sigma(bold(z))]_k >= 0$ for all $k$
2. 全ての要素の和は1である：$sum_(k=1)^K [sigma(bold(z))]_k = 1$

加えて、実装上の注意点として、ソフトマックス関数は指数関数を用いるため、$z_k$の値が大きい場合にオーバーフローが発生する可能性がある。この問題を回避するために、$bold(z)$の各要素からその最大値を引くことで数値的に安定化させることが一般的である。

::: box 【定理】ソフトマックス関数は定数の加法に不変
#### 主張
ソフトマックス関数$sigma(bold(z))$は、入力ベクトル$bold(z)$の各要素に定数$c in RR$を加えた場合でも出力が変わらない。
$$
[sigma(bold(z) + c bold(1))]_k = [sigma(bold(z))]_k quad (k = 1, 2, ..., K)
$$
ここで、$bold(1)$は全ての要素が1である$K$次元ベクトルである。

#### 証明
$$
[sigma(bold(z) + c bold(1))]_k &= frac(exp(z_k + c), sum_(j=1)^K exp(z_j + c)) \
&= frac(exp(z_k) exp(c), sum_(j=1)^K exp(z_j) exp(c)) \
&= frac(exp(z_k), sum_(j=1)^K exp(z_j)) \
&= [sigma(bold(z))]_k
$$
:::

この定理より、$c = - max_(k=1, 2, ..., K) z_k$とすることで、ソフトマックス関数の数値的安定化が可能となる。

## one-hot表現
クラス分類問題では、目的変数$y$を**one-hot表現**（one-hot representation）で表すことが多い。今ままで$y$はクラス集合$cal(C)$の要素として表されていたが、one-hot表現では$y$を$K$次元のベクトル$bold(y) in RR^K$で表す。具体的には、目的変数$y$がクラス$c_k$に属する場合、one-hot表現では以下のように表される。
$$
[bold(y)]_j &= cases(
1 quad & (j = k),
0 quad & (j eq.not k)
)\
bold(y) &= (0, 0, dots.c, 1, dots.c, 0)^top in RR^K
$$
すると、目的変数はスカラー値からベクトル値に変わる。この表現は、ある種の確率分布を表し、正解クラスが$1$で残りが$0$となるため、ソフトマックス関数の出力と比較しやすくなるという利点がある。今後はこのようにして目的変数を表すこととする。

すると、予測関数$f$はベクトルからベクトルを出力する関数となり、以下のように修正される。
$$
f(bold(x)) = sigma(bold(W) bold(x))
$$
つまり、目的変数$bold(y)$が確率分布で$K$次元ベクトルになったので予測関数はソフトマックス関数の出力そのものにすることができる。

## 最尤推定

::: box 【定義】尤度
ある確率モデルが与えられたとき、観測データ$cal(D) = {(bold(x)_(i), bold(y)_(i))}_{i=1}^n$が得られたとする。このとき、モデルのパラメータ$bold(W)$に対する**尤度**（likelihood）$L(bold(W))$は以下のように定義される。
$$
L(bold(W)) = product_(i=1)^n P(bold(y)_(i) | bold(x)_(i); bold(W))
$$
ここで、$P(bold(y)_(i) | bold(x)_(i); bold(W))$は説明変数$bold(x)_(i)$の元で目的変数$bold(y)_(i)$が観測される確率を表す。
:::

つまり尤度とはあるモデルとパラメータの元で観測データが得られる確率を表す。観測データはすでに得られているので、尤度を最大化（1に近づける）するようなパラメータ$bold(W)$を求めることが目的となる。この方法を**最尤推定**（maximum likelihood estimation）と呼ぶ。

さて、ロジスティック回帰モデルにおける尤度を計算してみよう。ある説明変数$bold(x)_i$において、パラメータ$W$の元で目的変数$bold(y)_i$が観測される確率は以下のように計算できる。
$$
P(hat(bold(y)) = bold(y)_(i) | bold(x)_(i); bold(W)) = [sigma(bold(W) bold(x)_(i))]_k
$$
ここで、$k$は目的変数$bold(y)_(i)$が1を持つ添字、つまり正解クラスである。目的変数$bold(y)_(i)$がone-hot表現であることを思い出すと、上式は以下のように書き換えられる。
$$
P(hat(bold(y)) = bold(y)_(i) | bold(x)_(i); bold(W)) = product_(k=1)^K [sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k)
$$
なぜなら
$$
[sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k) = cases(
[sigma(bold(W) bold(x)_(i))]_k quad & ([bold(y)_(i)]_k = 1),
1 quad & ([bold(y)_(i)]_k = 0)
)\
$$
であるからである。したがって、尤度$L(bold(W))$は以下のように計算できる。
$$
L(bold(W)) &= product_(i=1)^n P(hat(bold(y)) = bold(y)_(i) | bold(x)_(i); bold(W)) \
&= product_(i=1)^n product_(k=1)^K [sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k)
$$
最尤推定ではこの尤度$L(bold(W))$を最大化するようなパラメータ$bold(W)^ast$を求める。

さて、これを用いて損失関数を考えよう。このままだと指数が多重積分の形になっており計算が煩雑になるため、対数を取って対数尤度関数$log L(bold(W))$を考えることにする。対数関数は単調増加関数であり、正の数に対して1対1で対応するため、対数尤度関数を最大化することは尤度関数を最大化することと同値である。また、損失関数は最小化問題として定義されることが多いため、対数尤度関数の負の値を損失関数とする。以上で新しい損失関数$cal(L)_"CE" (bold(W))$を以下のように定義できる。
$$
cal(L)_"CE" (bold(W)) &= - log L(bold(W)) \
&= - log( product_(i=1)^n product_(k=1)^K [sigma(bold(W) bold(x)_(i))]_k^([bold(y)_(i)]_k) ) \
&= - sum_(i=1)^n sum_(k=1)^K [bold(y)_(i)]_k log [sigma(bold(W) bold(x)_(i))]_k
$$
ここで、$cal(L)_"CE" (bold(W))$は**交差エントロピー損失関数**（cross-entropy loss function）と呼ばれ、「エントロピー」という情報理論の概念からも導出できる。交差エントロピー損失関数は、モデルの予測分布$sigma(bold(W) bold(x)_(i))$と実際の分布$bold(y)_(i)$との間の差異を測る指標であり、これを最小化することはモデルの予測が実際のデータに近づくことを意味する。交差エントロピーは以下のように分解可能である。
$$
cal(L)^("CE")_i (bold(W)) :&= - sum_(k=1)^K [bold(y)_(i)]_k log [sigma(bold(W) bold(x)_(i))]_k\
cal(L)_"CE" (bold(W)) &= sum_(i=1)^n cal(L)^("CE")_i (bold(W))
$$
ここで、$cal(L)^("CE")_i (bold(W))$は$i$番目のデータ点に対する交差エントロピー損失を表す。したがって、交差エントロピーは分解可能目的である。

## 勾配降下法による最適化
交差エントロピー損失関数$cal(L)_"CE" (bold(W))$を最小化するパラメータ$bold(W)^ast$を求めるために、勾配降下法を用いる。まず、交差エントロピー損失関数の勾配を計算する必要がある。