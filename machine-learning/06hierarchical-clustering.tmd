# 階層的クラスタリング
非階層的クラスタリングではクラスタごとに上下の差や関係がないが、階層的クラスタリングではクラスタ間に上下関係や包含関係がある。例えば、動物をクラスタリングする場合、犬と猫は哺乳類に含まれ、哺乳類は動物に含まれる。このような包含関係を考慮することで、より詳細なクラスタリングが可能になる。

階層的クラスタリングには、主に凝集型（ボトムアップ）と分割型（トップダウン）の2つのアプローチがある。

1. **凝集（ボトムアップ）型**（Agglomerative Hierarchical Clustering）: 各データポイントを個別のクラスタとして開始し、最も近いクラスタ同士を繰り返し結合していく方法
2. **分割（トップダウン）型**（Divisive Hierarchical Clustering）: 全てのデータポイントを一つのクラスタとして開始し、最も異なるクラスタを繰り返し分割していく方法

## 凝集型階層的クラスタリング
ステップ$t >= 0$について、クラスタの集合を$cal(C)^((t)) = {C_1^((t)), C_2^((t)), ..., C_(K(t))^((t))}$と表す。ここで、$K(t)$はステップ$t$におけるクラスタ数である。さて、あるデータセット$cal(D) = {bold(x)_1, bold(x)_2, ..., bold(x)_n}$に対して考える。任意の二つのデータ点（ベクトル）に対しその距離を測る距離関数$d: RR^d times RR^d -> RR_(+)$を決める。さらに、任意の二つのクラスタ$C_i$と$C_j$に対しその距離を測るクラスタ間距離関数$"dist": cal(P)(RR^d) times cal(P)(RR^d) -> RR_(+)$を決める。この$"dist"$の定義についてはそのバリエーションで複数の方法がある。

さて、凝集型階層的クラスタリングは総じて、以下の手順で行われる。

::: box 凝集型階層的クラスタリングの基本的なアルゴリズム
1. 各データ点を個別のクラスタとして初期化する。すなわち、
    $$
    cal(C)^((0)) = { {1}, {2}, ..., {n} }
    $$
    とする。
2. ステップ$t$において、最も近い二つのクラスタ$C_(hat(i))^((t))$と$C_(hat(j))^((t))$を見つける。
    $$
    hat(i), hat(j) = op("argmin", limits: #true)_(i, j in {1, 2, ..., K(t)}, i < j) "dist"(C_i^((t)), C_j^((t)))
    $$
3. クラスタ$C_(hat(i))^((t))$と$C_(hat(j))^((t))$を結合し、新しいクラスタ$C_*^((t+1))$を作成する。
    $$
    C_*^((t+1)) = C_(hat(i))^((t)) union C_(hat(j))^((t))
    $$
4. クラスタ$C_(hat(i))^((t))$と$C_(hat(j))^((t))$を削除し、新しいクラスタ$C_*^((t+1))$を追加する。
    $$
    cal(C)^((t+1)) = (cal(C)^((t)) \\ {C_(hat(i))^((t)), C_(hat(j))^((t))}) union {C_*^((t+1))}
    $$
5. ステップ$t+1$におけるクラスタ数を$K(t+1) = K(t) - 1$とする。
6. $K(t+1) > 1$であれば、2に戻る。そうでなければ終了する。
:::

このアルゴリズムは、各ステップで最も"近い"クラスタ同士を結合していくことで、階層的なクラスタ構造を形成する。最終的に一つのクラスタになるまで繰り返すことで、**デンドログラム**（dendrogram）と呼ばれる木構造の図を得ることができる。

デンドログラムは、クラスタの結合過程を視覚的に表現し、どのクラスタがどの段階で結合されたかを示す。横軸に初期のデータ点、縦軸にクラスタ間の距離を取り、二つのクラスがある距離で結合されたことを示す。最終的に1つに統合されるが、途中の段階で切断することで、任意のクラスタ数に分割することができる。

## さまざまな距離関数
距離関数$d: RR^d times RR^d -> RR_(+)$としては、以下の条件を満たす必要がある。

- 非負性: $forall bold(x), bold(y) in RR^d, d(bold(x), bold(y)) >= 0$
- 一致性: $forall bold(x), bold(y) in RR^d, d(bold(x), bold(y)) = 0 <=> bold(x) = bold(y)$
- 対称性: $forall bold(x), bold(y) in RR^d, d(bold(x), bold(y)) = d(bold(y), bold(x))$
- 三角不等式: $forall bold(x), bold(y), bold(z) in RR^d, d(bold(x), bold(z)) <= d(bold(x), bold(y)) + d(bold(y), bold(z))$

代表的な距離関数としては、以下のようなものがある。
### ユークリッド距離
別名**L2ノルム距離**（L2 norm distance）とも呼ばれる。
$$
d(bold(x), bold(y)) = ||bold(x) - bold(y)||_2 = sqrt(sum_(i=1)^d (x_i - y_i)^2)
$$
### マンハッタン距離
別名**L1ノルム距離**（L1 norm distance）とも呼ばれる。
$$
d(bold(x), bold(y)) = ||bold(x) - bold(y)||_1 = sum_(i=1)^d |x_i - y_i|
$$
### コサイン類似度
コサイン類似度は距離関数ではなく類似度関数であるが、1から引くことで距離関数として利用できる。
$$
d(bold(x), bold(y)) = 1 - frac(bold(x)^top bold(y), ||bold(x)||_2 ||bold(y)||_2)
$$

これは三角不等式を満たさない場合があるため、厳密には距離関数ではないことに注意が必要である。

## さまざまなクラスタ間距離関数
クラスタ間距離関数$"dist": cal(P)(RR^d) times cal(P)(RR^d) -> RR_(+)$の違いはそのまま凝集型階層的クラスタリングの方法の違いになる。以下はここで紹介する手法の違いである。

|手法名|クラスタ間距離関数|クラスタの特徴|ノイズ/外れ値|弱点|性質|
|:---:|:---:|:---:|:---:|:---:|:---:|
|最短距離法（単連結法）|最短距離|細長い形状|弱い|鎖状クラスタ形成|単調|
|最長距離法（完全連結法）|最長距離|コンパクトな形状|強い|外れ値に敏感|単調|
|群平均法|平均距離|バランスの取れた形状|中程度|なし|単調|
|重心法|重心間距離|多様な形状|弱い|非単調な結合|非単調・ユークリッド距離|
|ウォード法|クラスタ内平方和の増加量|コンパクトな形状|かなり強い|ベクトル内スケールに敏感|単調・ユークリッド距離|

また。ここではベクトルを念頭に置いているが、最短距離法, 最長距離法, 群平均法は適切な距離関数を用いればベクトル以外の特徴量にも適用可能である。


### 最短距離法（単連結法）
二つのクラスタ$C_i$と$C_j$の距離を、それらのクラスタに属するデータ点間の最短距離として定義する。
$$
"dist"(C_i, C_j) = min_(a in C_i, b in C_j) d(bold(x)_a, bold(x)_b)
$$
クラスタが、クラスタ内の最も近いデータ点同士の距離に基づいて結合される。そのため、クラスタが細長く伸びた形状（平たく言えば鎖状）になる傾向がある。その結果、クラスタ内の最も遠い点の距離（これをクラスタの直径と呼ぶ）が大きくなることがある。

### 最長距離法（完全連結法）
二つのクラスタ$C_i$と$C_j$の距離を、それらのクラスタに属するデータ点間の最長距離として定義する。
$$
"dist"(C_i, C_j) = max_(a in C_i, b in C_j) d(bold(x)_a, bold(x)_b)
$$
クラスタが、クラスタ内の最も遠いデータ点同士の距離に基づいて結合される。そのため、クラスタが比較的コンパクトな形状（球状）になる傾向がある。しかし、外れ値に敏感であり、近そうな点があっても外れ値があると結合されないことがある。

### 群平均法
二つのクラスタ$C_i$と$C_j$の距離を、それらのクラスタに属する全てのデータ点間の平均距離として定義する。
$$
"dist"(C_i, C_j) &:= EE[d(bold(x)_a, bold(x)_b) | a in C_i, b in C_j]\
&= 1/(|C_i| |C_j|) sum_(a in C_i) sum_(b in C_j) d(bold(x)_a, bold(x)_b)
$$
クラスタが、クラスタ内の全てのデータ点間の距離の平均に基づいて結合される。そのため、クラスタが比較的バランスの取れた形状になる傾向がある。外れ値に対しても比較的ロバストである。

### 重心法
二つのクラスタ$C_i$と$C_j$の距離を、それらのクラスタの重心間の距離として定義する。各クラスタの重心は、そのクラスタに属するデータ点の平均ベクトルとして計算される。
$$
bold(mu)_i = 1/(|C_i|) sum_(a in C_i) bold(x)_a
$$
これを用いて
$$
"dist"(C_i, C_j) = d(bold(mu)_i, bold(mu)_j)
$$
と定義される。ほとんどの場合、距離関数$d$としてユークリッド距離が用いられる。重心が、外れ値に敏感であるため、外れ値が存在する場合には注意が必要である。また、状態によっては統合が進むとクラスタ間距離が減少することがあり、非単調なクラスタ結合が発生する可能性がある。

### ウォード法
二つのクラスタ$C_i$と$C_j$の距離を、クラスタ内平方和の増加量として定義する。クラスタ内平方和は以下で定義されたものであった。なお、ウォード法では距離関数$d$としてユークリッド距離が用いられる。
$$
cal(J) = sum_(i=1)^K sum_(j in C_i) ||bold(x)_j - bold(mu)_i||_2^2
$$
ここで$bold(mu)_i$はクラスタ$C_i$の重心ベクトルである。また、一つのクラスタ$C_k$のクラスタ内平方和も定義しておく。
$$
V(C_k) = sum_(j in C_k) ||bold(x)_j - bold(mu)_k||_2^2
$$
当然、全体のクラスタ内平方和は各クラスタのクラスタ内平方和の和である。このとき、あることなる二つのクラスタ$C_i$と$C_j$を結合したときの全体のクラスタ内平方和の増加量$Delta (C_i, C_j)$を考えてみると
$$
Delta (C_i, C_j) &= (V(C_i union C_j) + sum_(k eq.not i, j) V(C_k)) - sum_k V(C_k)\
&= V(C_i union C_j) - (V(C_i) + V(C_j))
$$
である。これを定義に則して展開してみると、$C_i union C_j$の重心を$bold(mu)_(i union j)$を用いて
$$
Delta (C_i, C_j) &= sum_(a in C_i union C_j) ||bold(x)_a - bold(mu)_(i union j)||_2^2 - (sum_(a in C_i) ||bold(x)_a - bold(mu)_i||_2^2 + sum_(b in C_j) ||bold(x)_b - bold(mu)_j||_2^2)\
&= sum_(a in C_i) ||bold(x)_a - bold(mu)_(i union j)||_2^2 + sum_(b in C_j) ||bold(x)_b - bold(mu)_(i union j)||_2^2 - (sum_(a in C_i) ||bold(x)_a - bold(mu)_i||_2^2 + sum_(b in C_j) ||bold(x)_b - bold(mu)_j||_2^2)\
&= underbracket(sum_(a in C_i) (||bold(x)_a - bold(mu)_(i union j)||_2^2 - ||bold(x)_a - bold(mu)_i||_2^2), I_i) + underbracket(sum_(b in C_j) (||bold(x)_b - bold(mu)_(i union j)||_2^2 - ||bold(x)_b - bold(mu)_j||_2^2), I_j)
$$
となる。ここで、$I_i$について考える。
$$
I_i &= sum_(a in C_i) (||bold(x)_a||^2_2 - 2 bold(x)_a^top bold(mu)_(i union j) + ||bold(mu)_(i union j)||^2_2 - ||bold(x)_a||^2_2 + 2 bold(x)_a^top bold(mu)_i - ||bold(mu)_i||^2_2)\
&= sum_(a in C_i) (- 2 bold(x)_a^top bold(mu)_(i union j) + ||bold(mu)_(i union j)||^2_2 + 2 bold(x)_a^top bold(mu)_i - ||bold(mu)_i||^2_2)\
&= - 2 bold(mu)_(i union j)^top sum_(a in C_i) bold(x)_a + |C_i| ||bold(mu)_(i union j)||^2_2 + 2 bold(mu)_i^top sum_(a in C_i) bold(x)_a - |C_i| ||bold(mu)_i||^2_2\
&= - 2 bold(mu)_(i union j)^top (|C_i| bold(mu)_i) + |C_i| ||bold(mu)_(i union j)||^2_2 + 2 bold(mu)_i^top (|C_i| bold(mu)_i) - |C_i| ||bold(mu)_i||^2_2\
&= |C_i| (- 2 bold(mu)_(i union j)^top bold(mu)_i + ||bold(mu)_(i union j)||^2_2 + ||bold(mu)_i||^2_2)\
&= |C_i| ||bold(mu)_i - bold(mu)_(i union j)||_2^2
$$
である。途中、
$$
bold(mu)_i = 1/(|C_i|) sum_(a in C_i) bold(x)_a thin <=> thin sum_(a in C_i) bold(x)_a = |C_i| bold(mu)_i
$$
を用いた。これと同様にして$I_j$についても計算すると
$$
I_j = |C_j| ||bold(mu)_j - bold(mu)_(i union j)||_2^2
$$
である。さて、
$$
bold(mu)_(i union j) &= 1/(|C_i union C_j|) (sum_(a in C_i union C_j) bold(x)_a)\
&= 1/(|C_i| + |C_j|) (sum_(a in C_i) bold(x)_a + sum_(b in C_j) bold(x)_b)\
&= (|C_i| bold(mu)_i + |C_j| bold(mu)_j)/(|C_i| + |C_j|) 
$$
なので、
$$
bold(mu)_i - bold(mu)_(i union j) &= (|C_i|bold(mu)_i + |C_j|bold(mu)_i)/(|C_i| + |C_j|) - (|C_i| bold(mu)_i + |C_j| bold(mu)_j)/(|C_i| + |C_j|)\
&= (|C_j|)/(|C_i| + |C_j|) (bold(mu)_i - bold(mu)_j)\
bold(mu)_j - bold(mu)_(i union j) &= (|C_i|bold(mu)_j + |C_j|bold(mu)_j)/(|C_i| + |C_j|) - (|C_i| bold(mu)_i + |C_j| bold(mu)_j)/(|C_i| + |C_j|)\
&= (|C_i|)/(|C_i| + |C_j|) (bold(mu)_j - bold(mu)_i)
$$
となる。これらを用いて、最終的に
$$
Delta (C_i, C_j) &= |C_i| ||bold(mu)_i - bold(mu)_(i union j)||_2^2 + |C_j| ||bold(mu)_j - bold(mu)_(i union j)||_2^2\
&= |C_i| ( (|C_j|)/(|C_i| + |C_j|) )^2 ||bold(mu)_i - bold(mu)_j||_2^2 + |C_j| ( (|C_i|)/(|C_i| + |C_j|) )^2 ||bold(mu)_j - bold(mu)_i||_2^2\
&= (|C_i| |C_j|)/(|C_i| + |C_j|) ||bold(mu)_i - bold(mu)_j||_2^2
$$
となる。$Delta (C_i, C_j)$が大きいということは、統合によってクラスタ内平方和が大きく増加することになり、分散が増加することを意味する。したがって、ウォード法では、この$Delta (C_i, C_j)$が最小となるクラスタ同士を結合する。これはすなわち$C_i$と$C_j$のクラスタ間距離関数として$Delta (C_i, C_j)$が採用できることを意味する。

しかし、そのままではスケールと単点クラスタにおける振る舞いが問題になる。$t=0$、つまり各データ点が個別のクラスタであるとき、任意の二つのクラスタ$C_i = {a}$と$C_j = {b}$に対して
$$
Delta ({a}, {b}) = 1/2 ||bold(x)_a - bold(x)_b||_2^2
$$
となる。これを2点間のユークリッド距離$||bold(x)_a - bold(x)_b||_2$に一致させるために、ウォード法ではクラスタ間距離関数を以下のように定義する。
$$
"dist"(C_i, C_j) = sqrt( 2 Delta (C_i, C_j) ) = sqrt( (2 |C_i| |C_j|)/(|C_i| + |C_j|) ) ||bold(mu)_i - bold(mu)_j||_2
$$
動作上、ユークリッド距離に一致させる必要は特にはないが、他の方法との比較を容易にするためにこのように定義されることが多い。ウォード法は、クラスタ内平方和を最小化することを目的としているため、比較的均一でコンパクトなクラスタを形成する傾向がある。また、外れ値に対しても比較的ロバストである。
